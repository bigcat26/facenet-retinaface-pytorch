{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image对象转cv2(np.adarray)\n",
    "\n",
    "```python\n",
    "img = Image.open(path)\n",
    "img_array = np.array(img)\n",
    "```\n",
    "\n",
    "### cv2(np.adarray)转Image对象\n",
    "\n",
    "```python\n",
    "img = cv2.imread(path)\n",
    "img_Image = Image.fromarray(np.uint8(img))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cv2读取默认是BGR，需要转换才能正确显示\n",
    "img = cv2.imread(\"img/obama.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "fimg = np.array(Image.open(\"img/obama.jpg\"), np.float32)\n",
    "img = fimg.astype(np.uint8)\n",
    "print(fimg.dtype, img.dtype)\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imshow内部的参数类型可以分为两种\n",
    "\n",
    "- 当输入矩阵是uint8类型的时候，此时imshow显示图像的时候，会认为输入矩阵的范围在`0-255`之间\n",
    "- 当输入矩阵是double类型的时候，那么imshow会认为输入矩阵的范围在`0-1`\n",
    "\n",
    "因此, 需要使用astype将float32转换为uint8，否则会出现一片白色([python中opencv imshow函数显示一片白色原因](https://blog.csdn.net/lyl771857509/article/details/80143134))。如果使用int8，会出现另一个问题：\n",
    "\n",
    "```\n",
    "resize.cpp:3787: error: (-215:Assertion failed) func != 0 in function 'cv::hal::resize'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils\n",
    "\n",
    "fimg2 = utils.letterbox_image(img, [600, 800])\n",
    "img2 = fimg2.astype(np.uint8)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = utils.preprocess_input(fimg)\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from retinaface import Retinaface\n",
    "from utils import utils\n",
    "\n",
    "retinaface = Retinaface(cuda=torch.cuda.is_available())\n",
    "\n",
    "img = np.array(Image.open(\"img/twins.jpg\"), np.float32)\n",
    "result = retinaface.face_detect(img)\n",
    "print(f'faces detected: {result.shape[0]}')\n",
    "crop_imgs = []\n",
    "crop_feat = []\n",
    "aligned_imgs = []\n",
    "aligned_feat = []\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    # print(result[i][0:2], result[i][2:4])\n",
    "    # print(result[i])\n",
    "    \n",
    "    b = np.array(result[i], np.int32)\n",
    "    cx = b[0]\n",
    "    cy = b[1] - 10\n",
    "    conf = str(result[i][4])\n",
    "    cropped = utils.crop_npimage(img, b)\n",
    "    crop_imgs.append(cropped)\n",
    "    cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "    cv2.putText(img, conf, (cx, cy), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "    cv2.circle(img, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "    cv2.circle(img, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "    cv2.circle(img, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "    cv2.circle(img, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "    cv2.circle(img, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "    cfeat = retinaface.face_feature(cropped)\n",
    "    crop_feat.append(cfeat)\n",
    "\n",
    "    # 将landmark坐标从全图位置转换为人脸图原点偏移位置\n",
    "    landmark = np.reshape(b[5:],(5,2)) - np.array([int(b[0]),int(b[1])])\n",
    "    # 做人脸对齐\n",
    "    aligned, _ = utils.Alignment_1(cropped, landmark)\n",
    "    aligned_imgs.append(aligned)\n",
    "    afeat = retinaface.face_feature(aligned)\n",
    "    aligned_feat.append(afeat)\n",
    "    # print(f'shape type:{type(cfeat)} c:{cfeat.shape} cl:{len(cfeat)} a:{afeat.shape}')\n",
    "    print(f'cfeat & afeat distance: {utils.face_distance(cfeat, afeat, axis=0)}')\n",
    "\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "nrow = 2\n",
    "ncol = 8\n",
    "# fig, axs = plt.subplots(3, 1, constrained_layout=True, figsize=(20, 20))\n",
    "fig = plt.figure(3, constrained_layout=True, figsize=(20, 20))\n",
    "sf1, sf2, sf3 = fig.subfigures(3, 1)\n",
    "sf1.suptitle('output')\n",
    "sf2.suptitle('cropped')\n",
    "sf3.suptitle('aligned')\n",
    "\n",
    "plt_out = sf1.subplots(1, 1)\n",
    "plt_out.imshow(img)\n",
    "plt_crop = sf2.subplots(nrow, ncol)\n",
    "plt_align = sf3.subplots(nrow, ncol)\n",
    "\n",
    "# fig = plt.figure(figsize=[10.0, 10.0])\n",
    "# fig.add_subplot(i,2,1)\n",
    "# plt.imshow(img)\n",
    "\n",
    "for i in range(len(crop_imgs)):\n",
    "    if (i < nrow * ncol):\n",
    "        plt_crop[i // ncol, i % ncol].imshow(crop_imgs[i].astype(np.uint8))\n",
    "        plt_align[i // ncol, i % ncol].imshow(aligned_imgs[i].astype(np.uint8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(20, 20))\n",
    "subfig = fig.subfigures(2, 1)\n",
    "axs = subfig[0].subplots(nrow, ncol)\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    r = result[i]\n",
    "    # 将landmark坐标从全图位置转换为人脸图原点偏移位置\n",
    "    landmark = np.reshape(r[5:],(5,2)) - np.array([int(r[0]),int(r[1])])\n",
    "    # 做人脸对齐\n",
    "    aligned, _ = utils.Alignment_1(crop_imgs[i], landmark)\n",
    "    axs[i // ncol, i % ncol].imshow(aligned.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from retinaface import Retinaface\n",
    "from utils import utils\n",
    "\n",
    "retinaface = Retinaface(cuda=torch.cuda.is_available())\n",
    "\n",
    "def extract_feature(image_file):\n",
    "    img = np.array(Image.open(image_file), np.float32)\n",
    "    img = img[:,:,:3]\n",
    "\n",
    "    result = retinaface.face_detect(img)\n",
    "    if len(result) > 1:\n",
    "        raise ('more than one face was detected')\n",
    "\n",
    "    result = result[0]\n",
    "    # print(result)\n",
    "    bbox = result[0:4].astype(np.uint)\n",
    "    conf = result[4]\n",
    "    lm = result[5:].astype(np.uint).reshape(5, 2)\n",
    "\n",
    "    # print(bbox)\n",
    "    # print(conf)\n",
    "    # print(lm)\n",
    "\n",
    "    cimg = utils.crop_npimage(img, bbox)\n",
    "    # cv2.imwrite(\"c1.jpg\", cv2.cvtColor(cimg, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # 将landmark坐标从全图位置转换为人脸图原点偏移位置\n",
    "    lm = lm - np.array([int(bbox[0]),int(bbox[1])])\n",
    "    align, _ = utils.Alignment_1(cimg, lm)\n",
    "    # cv2.imwrite(\"c2.jpg\", cv2.cvtColor(align, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    feat0 = retinaface.face_feature(cimg)\n",
    "\n",
    "    feat = retinaface.face_feature(align)\n",
    "    return cimg, align, feat\n",
    "\n",
    "origin = '../dataset/manual/face/lyf1.jpg'\n",
    "comparison = [\n",
    "    '../dataset/manual/face/jacky1.png', \n",
    "    '../dataset/manual/face/jacky2.png', \n",
    "    '../dataset/manual/face/jacky3.png',\n",
    "    '../dataset/manual/face/jacky4.png',\n",
    "    '../dataset/manual/face/jacky5.png',\n",
    "    '../dataset/manual/face/sa1.jpg', \n",
    "    '../dataset/manual/face/sa2.jpg', \n",
    "    '../dataset/manual/face/sa3.jpg', \n",
    "    '../dataset/manual/face/gillian2.jpg', \n",
    "    '../dataset/manual/face/gillian3.jpg', \n",
    "    '../dataset/manual/face/gillian4.jpg', \n",
    "    '../dataset/manual/face/gillian5.jpg', \n",
    "    '../dataset/manual/face/gillian6.jpg', \n",
    "    '../dataset/manual/face/gillian7.jpg', \n",
    "    '../dataset/manual/face/lyf1.jpg', \n",
    "    '../dataset/manual/face/lyf2.jpg', \n",
    "    '../dataset/manual/face/lyf3.jpg', \n",
    "    '../dataset/manual/face/lyf4.jpg', \n",
    "    '../dataset/manual/face/lyf5.jpg', \n",
    "    '../dataset/manual/face/ycl1.jpg', \n",
    "]\n",
    "\n",
    "fig = plt.figure(2, constrained_layout=True, figsize=(20, 20))\n",
    "sf1, sf2 = fig.subfigures(2, 1)\n",
    "sf1.suptitle('input')\n",
    "sf2.suptitle('comparison')\n",
    "\n",
    "cimg, aimg, origin_feat = extract_feature(origin)\n",
    "\n",
    "ax = sf1.subplots(1, 1)\n",
    "ax.imshow(aimg.astype(np.uint8))\n",
    "\n",
    "ax = sf2.subplots(len(comparison) // 4 + 1, 4)\n",
    "for i, f in enumerate(comparison):\n",
    "    ci, ai, feat = extract_feature(f)\n",
    "    dist = utils.face_distance(origin_feat, feat, 0)\n",
    "    ci = ci.astype(np.uint8)\n",
    "    ai = ai.astype(np.uint8)\n",
    "    ax[i // 4][i % 4].set_title(str(round(dist, 3)))\n",
    "    ax[i // 4][i % 4].imshow(ai)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
